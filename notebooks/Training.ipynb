{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddc4eea4-f0b1-42ec-9ef7-b29eacc9d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pymap3d as pm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from globenn.model import *\n",
    "\n",
    "# For reproducibility\n",
    "os.environ['PYTHONHASHSEED']= '123'\n",
    "np.random.seed(123)\n",
    "plt.style.use(\"../hatsyim.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f96e2-48bb-404f-ba4a-c38bdc553099",
   "metadata": {},
   "source": [
    "# Obtaining the 3-D global mantle model\n",
    "\n",
    "Please refer to the official [GLAD-M25 repository](https://github.com/caiociardelli/gladm25) to obtain the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb3fba-f8d6-4fa3-9b99-1672f800cbe9",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039ab8d9-ca44-4a7a-bb0f-da894b37f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = '/home/taufikmh/KAUST/spring_2022/global_pinns/data/glad-m25-vp-0.0-n4.nc'\n",
    "data = xr.open_dataset(path_file)\n",
    "\n",
    "offline = False\n",
    "old = False\n",
    "\n",
    "# Subsample the model along the depth, latitude, and longitude directions\n",
    "if offline and old:\n",
    "    dep_ini, dep_inc = 11, 4\n",
    "    lat_ini, lat_inc = 0, 8\n",
    "    lon_ini, lon_inc = 0, 8\n",
    "else:\n",
    "    dep_ini, dep_inc = 11, 2\n",
    "    lat_ini, lat_inc = 0, 4\n",
    "    lon_ini, lon_inc = 0, 4    \n",
    "\n",
    "# Input variables\n",
    "vpv = data.variables['vpv'].values[dep_ini::dep_inc, lat_ini::lat_inc, lon_ini::lon_inc]\n",
    "vph = data.variables['vph'].values[dep_ini::dep_inc, lat_ini::lat_inc, lon_ini::lon_inc]\n",
    "longitude = data.variables['longitude'].values[lon_ini::lon_inc]\n",
    "latitude = data.variables['latitude'].values[lat_ini::lat_inc]\n",
    "depth = data.variables['depth'].values[dep_ini::dep_inc]\n",
    "\n",
    "dep_dim = vpv.shape[0]\n",
    "lat_dim = vpv.shape[1]\n",
    "lon_dim = vpv.shape[2]\n",
    "\n",
    "# Projection\n",
    "LAT, ALT, LON = np.meshgrid(latitude, -1e3*depth, longitude)\n",
    "x, y, z = pm.geodetic2ecef(LAT, LON, ALT)\n",
    "_, DEP, _ = np.meshgrid(latitude, depth, longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050ab70-2e60-4a7d-bf24-aa4203a56c9d",
   "metadata": {},
   "source": [
    "# Medium and Saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a913a9-0ea4-4582-917e-212593e7829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all stations\n",
    "ISCall = pd.read_csv('../data/stations.csv')\n",
    "ISCall = ISCall.rename(columns={\"X\":\"LON\", 'Y':'LAT'})\n",
    "ISCarray = ISCall[ISCall['description'].str.contains('to 2021')]\n",
    "ISCarrLon = ISCarray['LON'][::5]\n",
    "ISCarrLat = ISCarray['LAT'][::5]\n",
    "\n",
    "USarray = pd.read_excel('../data/_US-TA-StationList.xls')\n",
    "USarrLon = USarray['LON'][::1]\n",
    "USarrLat = USarray['LAT'][::1]\n",
    "\n",
    "# Concatenate the two receiver group\n",
    "AllLon = np.hstack((USarrLon, ISCarrLon))\n",
    "AllLat = np.hstack((USarrLat, ISCarrLat))\n",
    "\n",
    "# Model specifications\n",
    "ear_rad = 6371\n",
    "rec_typ = 'US_array'\n",
    "if rec_typ == 'ISC_array':\n",
    "    print(rec_typ)\n",
    "    lat_sou = np.array([latitude.flat[np.abs(latitude - i).argmin()] for i in ISCarrLat])\n",
    "    lon_sou = np.array([longitude.flat[np.abs(longitude - i).argmin()] for i in ISCarrLon])\n",
    "elif rec_typ == 'US_array':\n",
    "    print(rec_typ)\n",
    "    lat_sou = np.array([latitude.flat[np.abs(latitude - i).argmin()] for i in USarrLat])\n",
    "    lon_sou = np.array([longitude.flat[np.abs(longitude - i).argmin()] for i in USarrLon])\n",
    "dep_sou = depth.flat[np.abs(depth - 0).argmin()]\n",
    "\n",
    "# Plotting coordinates\n",
    "xx = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.cos(np.radians(180+LON))/(1e3)\n",
    "yy = (ear_rad - DEP) * np.sin(np.radians(LAT+90)) * np.sin(np.radians(180+LON))/(1e3)\n",
    "zz = DEP * np.cos(np.radians(LAT+90)) / (1e3)\n",
    "xx_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.cos(np.radians(180+lon_sou))/(1e3)\n",
    "yy_s = (ear_rad - dep_sou) * np.sin(np.radians(lat_sou+90)) * np.sin(np.radians(180+lon_sou))/(1e3)\n",
    "\n",
    "# Coordinates setup\n",
    "sx, sy, sz = pm.geodetic2ecef(lat_sou, lon_sou, -1e3*dep_sou)\n",
    "\n",
    "# Rescale\n",
    "x,y,z = x/(ear_rad*1e3), y/(ear_rad*1e3), z/(ear_rad*1e3)\n",
    "sx, sy, sz = sx/(ear_rad*1e3), sy/(ear_rad*1e3), sz/(ear_rad*1e3)\n",
    "X,Y,Z = x,y,z\n",
    "\n",
    "sou_idx = np.where((np.isclose(x.reshape(-1,1), sx)) & (np.isclose(y.reshape(-1,1), sy)) & (np.isclose(z.reshape(-1,1), sz)))[0]\n",
    "sx, sy, sz = x.reshape(-1,1)[sou_idx], y.reshape(-1,1)[sou_idx], z.reshape(-1,1)[sou_idx]\n",
    "\n",
    "# For plotting only\n",
    "x_plot,y_plot,z_plot = x.reshape(-1,1)*ear_rad/1000, y.reshape(-1,1)*ear_rad/1000, z.reshape(-1,1)*ear_rad/1000\n",
    "num_pts = x.size\n",
    "\n",
    "PROJECT_NAME='glad-m25'\n",
    "\n",
    "# Saving path\n",
    "model_path = \"./../saves/\" + PROJECT_NAME\n",
    "figures_path = model_path + '/'\n",
    "checkpoints_path = figures_path + 'checkpoints' + '/'\n",
    "predictions_path = figures_path + 'predictions' + '/'\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(checkpoints_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(predictions_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3eb15ad-b325-4c5c-8b06-fd4248455de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_first = False\n",
    "single_source = True\n",
    "permutate = False\n",
    "\n",
    "num_epo = int(2001)\n",
    "num_blo = 21 #20\n",
    "coo_sys = 'cartesian'\n",
    "vel_sha = 'sphere'\n",
    "vel_typ = 'gladm25'\n",
    "num_neu = 512\n",
    "lea_rat = 5e-6\n",
    "act_fun = torch.nn.ELU\n",
    "bat_siz = 512 #num_pts // 100\n",
    "ada_wei = False\n",
    "vel_sca = 1\n",
    "opt_fun = torch.optim.Adam\n",
    "dev_typ = \"cuda\"\n",
    "nor_typ = \"MinMax\"\n",
    "bac_vel = 10 #6 #10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a019e42b-8566-4b45-9df2-b6e54d71c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity_function(vel_sha='sphere', vel_typ='homogeneous', vel_ini=5, x=X, y=Y, z=Z):\n",
    "    if vel_sha=='sphere':\n",
    "        # inhomogeneous velocity without gaps\n",
    "        vel_inh = vel_ini - (x**2 + z**2 + y**2)\n",
    "        vel_inh[vel_inh<2.**2] = np.NaN\n",
    "        if vel_typ=='inhomogeneous':  \n",
    "            # velocity without gaps\n",
    "            vel_all = np.copy(vel_inh)\n",
    "            \n",
    "            # velocity with gaps\n",
    "            vel_gap = np.copy(vel_all)\n",
    "            vel_gap[vel_inh>2.1**2] = np.NaN\n",
    "        elif vel_typ == 'random':\n",
    "            # velocity without gaps\n",
    "            from scipy.ndimage import gaussian_filter\n",
    "            vel_all = gaussian_filter(18*np.random.random((nx,ny,nz))**3, sigma=6)\n",
    "            vel_all[np.isnan(vel_inh)] = np.NaN\n",
    "\n",
    "            # velocity with gaps\n",
    "            vel_gap = np.copy(vel_all)\n",
    "            vel_gap[vel_inh>2.1**2] = np.NaN\n",
    "        elif vel_typ == 'homogeneous':\n",
    "            # velocity without gaps\n",
    "            vel_all = vel_ini*np.ones_like(vel_inh)\n",
    "            vel_all[np.isnan(vel_inh)] = np.NaN\n",
    "            \n",
    "            # velocity with gaps\n",
    "            vel_tmp = np.copy(vel_inh)\n",
    "            vel_tmp[vel_inh>2.1**2] = np.NaN\n",
    "            vel_gap = np.copy(vel_all)\n",
    "            vel_gap[np.isnan(vel_tmp)] = np.NaN\n",
    "        elif vel_typ == 'radial':\n",
    "            vel_inh = vp\n",
    "            vel_all = vp\n",
    "            vel_gap = vp_gap\n",
    "        elif vel_typ == 'constant_radial':\n",
    "            vel_inh = 5*np.ones_like(vp)\n",
    "            vel_all = 5*np.ones_like(vp)\n",
    "            vel_gap = np.where(DEP<3000, vel_all, np.nan)\n",
    "        elif vel_typ == 'gladm25':\n",
    "            vel_inh = vpv\n",
    "            vel_all = vpv\n",
    "            vel_gap = vpv\n",
    "            \n",
    "            \n",
    "    return vel_inh, vel_all, vel_gap\n",
    "\n",
    "# call function\n",
    "vel_inh, vel_all, vel_gap = velocity_function(vel_typ=vel_typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d22f7-3bf3-4fd7-9e1a-765f03d340b9",
   "metadata": {},
   "source": [
    "# Training Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c97f8bcf-8a1a-4595-819d-dd0f1b285fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16405116 2734186\n",
      "(2234,)\n",
      "after\n",
      "[1309567  847284  847284 ...  248151  248151 1837617]\n",
      "[ 0.12723418 -0.12519088 -0.12519088 ... -0.28321096 -0.28321096\n",
      " -0.18222106]\n",
      "[-0.90531814 -0.8907796  -0.8907796  ... -0.12609363 -0.12609363\n",
      " -0.25080577]\n",
      "[0.40430745 0.43579215 0.43579215 ... 0.94772875 0.94772875 0.94772875]\n",
      "[[ 0.12723418]\n",
      " [-0.12519088]\n",
      " [-0.12519088]\n",
      " ...\n",
      " [-0.28321096]\n",
      " [-0.28321096]\n",
      " [-0.18222106]] [[-0.90531814]\n",
      " [-0.8907796 ]\n",
      " [-0.8907796 ]\n",
      " ...\n",
      " [-0.12609363]\n",
      " [-0.12609363]\n",
      " [-0.25080577]] [[0.40430745]\n",
      " [0.43579215]\n",
      " [0.43579215]\n",
      " ...\n",
      " [0.94772875]\n",
      " [0.94772875]\n",
      " [0.94772875]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2817/618570644.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sids = np.asarray(sids)[:,0]\n",
      "/tmp/ipykernel_2817/618570644.py:42: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  sids = sids[sids.astype(bool)].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# define receiver coordinates\n",
    "xR, yR, zR = X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)\n",
    "\n",
    "# define source coordinates\n",
    "if single_source:\n",
    "    xS, yS, zS = sx[0]*np.ones_like(X.reshape(-1,1)), sy[0]*np.ones_like(X.reshape(-1,1)), sz[0]*np.ones_like(X.reshape(-1,1))\n",
    "else:\n",
    "    xS, yS, zS = np.tile(sx,(X.size//sx.shape[0]+1, 1))[:X.size].reshape(-1,1), np.tile(sy,(Y.size//sy.shape[0]+1,1))[:Y.size].reshape(-1,1), np.tile(sz,(Z.size//sz.shape[0]+1,1))[:Z.size].reshape(-1,1)\n",
    "\n",
    "# define inputs and output\n",
    "Xp = np.hstack((xS, yS, zS, xR, yR, zR))\n",
    "yp = vel_gap.reshape(-1,1)\n",
    "\n",
    "# input for database\n",
    "Xb = np.copy(Xp)\n",
    "yb = np.copy(yp)\n",
    "\n",
    "print(Xb.size, lat_dim*dep_dim*lon_dim)\n",
    "\n",
    "if permutate:\n",
    "    perm_idx = torch.randperm(X.size).numpy()\n",
    "    Xb[:, :3] = Xb[perm_idx, :3]\n",
    "\n",
    "# random sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xp[np.logical_not(np.isnan(yp))[:,0]], \n",
    "    yp[np.logical_not(np.isnan(yp))].reshape(-1,1), \n",
    "    test_size=0.1,\n",
    "    random_state=1335\n",
    ")\n",
    "\n",
    "# find source location id in X_star\n",
    "X_starf = [X_train[:,3].reshape(-1,1), X_train[:,4].reshape(-1,1), X_train[:,5].reshape(-1,1)]\n",
    "\n",
    "# sids,_ = np.where((np.isclose(X_starf[0], sx)) & (np.isclose(X_starf[1], sy)) & (np.isclose(X_starf[2], sz)))\n",
    "\n",
    "sids = [np.where((np.isclose(X_starf[0], sx[i], atol=1e-16)) & (np.isclose(X_starf[1], sy[i], atol=1e-16)) & (np.isclose(X_starf[2], sz[i], atol=1e-16))) for i in range(len(sx))]\n",
    "sids = np.asarray(sids)[:,0]\n",
    "\n",
    "print(sids.shape)\n",
    "\n",
    "sids = sids[sids.astype(bool)].astype(int)\n",
    "\n",
    "print(\"after\")\n",
    "print(sids)\n",
    "print(X_starf[0][sids,0])\n",
    "print(X_starf[1][sids,0])\n",
    "print(X_starf[2][sids,0])\n",
    "print(sx,sy,sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91fa84bb-d31c-483f-8ec1-be7025381f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# from globenn.utils import *\n",
    "database = database(model_path,\n",
    "         VelocityFunction(),\n",
    "         create=False,\n",
    "         Numsamples=int(1e3),\n",
    "         randomDist=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74015f1b-3e09-4fa1-9e3c-d64db0a432ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epo = int(2001)\n",
    "num_blo = 21 #20\n",
    "coo_sys = 'cartesian'\n",
    "vel_sha = 'sphere'\n",
    "vel_typ = 'gladm25'\n",
    "num_neu = 512\n",
    "lea_rat = 5e-6\n",
    "act_fun = torch.nn.ELU\n",
    "bat_siz = 128 #num_pts // 100\n",
    "ada_wei = False\n",
    "vel_sca = 1\n",
    "opt_fun = torch.optim.Adam\n",
    "dev_typ = \"cuda\"\n",
    "nor_typ = \"MinMax\"\n",
    "bac_vel = 10 #6 #10.5\n",
    "\n",
    "offline, old = False, False\n",
    "\n",
    "# Model parameters\n",
    "params = {}\n",
    "params['model_path'] = model_path\n",
    "params['device'] = dev_typ\n",
    "params['mixed_precision'] = False\n",
    "params['num_blocks'] = num_blo\n",
    "params['act_function'] = act_fun\n",
    "params['num_points'] = 1e4\n",
    "params['batch_size'] = bat_siz\n",
    "params['val_percentage'] = 10\n",
    "params['num_epochs'] = num_epo\n",
    "params['sampling_bounds'] = [0.1,0.9]\n",
    "params['log_frequency'] = 1\n",
    "params['save_frequency'] = 10\n",
    "params['learning_rate'] = lea_rat\n",
    "params['use_scheduler'] = True\n",
    "\n",
    "PROJECT_NAME='new-section-1'\n",
    "\n",
    "# Saving path\n",
    "model_path = \"./../saves/\" + PROJECT_NAME\n",
    "figures_path = model_path + '/'\n",
    "checkpoints_path = figures_path + 'checkpoints' + '/'\n",
    "predictions_path = figures_path + 'predictions' + '/'\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(checkpoints_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(predictions_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a95caf-1f38-4f65-aa6d-ed04adc2cea9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17b612aa-9b3f-4cf0-b110-7f764f2657d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch = 1 -- Training loss = 1.4412e+02 -- Validation loss = 3.5440e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(params)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/trained/model_sub1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(inputs)\n\u001b[1;32m    305\u001b[0m     loss_value, wv  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meikonal_loss(labels,inputs,outputs,torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m--> 308\u001b[0m \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/globenn/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/globenn/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from globenn.model import *\n",
    "\n",
    "model = Model(params)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7146947-ad84-4b3a-ba3b-0045f56c1803",
   "metadata": {},
   "source": [
    "# Test and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104bccc9-2515-4395-8ed8-0c05c43b2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = torch.utils.data.DataLoader(\n",
    "    torch.from_numpy(Xb).to(torch.float).to(torch.device('cuda')),\n",
    "    batch_size=int(Xb.shape[0]//10)\n",
    "    )\n",
    "T_pred = model.traveltimes(Xt, projection=False, normalization=True).cpu().reshape(vel_all.shape)\n",
    "V_pred = model.velocity(Xt, projection=False, normalization=True).cpu().reshape(vel_all.shape)\n",
    "    \n",
    "T_pred, V_pred = T_pred.detach().numpy(), V_pred.detach().numpy()\n",
    "\n",
    "# numpy to hdf5\n",
    "predictions = [\n",
    "    T_pred,\n",
    "    V_pred\n",
    "    \n",
    "]\n",
    "name = [\n",
    "    'T_pred',\n",
    "    'V_pred'\n",
    "]\n",
    "print(np.min(T_pred) , np.min(T_pred)==0)\n",
    "for i in range(len(predictions)):\n",
    "    h5f = h5py.File(predictions_path + name[i]+'.h5', 'w')\n",
    "    h5f.create_dataset(name[i], data=predictions[i])\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52aeb61-50ef-498f-a4a4-abe2e97e76ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
